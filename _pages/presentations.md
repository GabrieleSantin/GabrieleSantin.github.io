---
layout: archive
title: "Presentations"
permalink: /presentations/
author_profile: true
---

This page contains some conference presentations and posters.


<font size="5">
<table style="border:none;">

### Interpolation with the polynomial kernes

<tr style="border:none;">

<td width="25%" style="border:none;">

<img src="https://GabrieleSantin.github.io/images/ATMA2023_poster.png" style="padding-top: 7px;display: block;margin-right:35px;" width="90%">

</td>

<td width="65%" style="border:none;">
The polynomial kernels are widely used in machine learning and they are one of the default choices to develop kernel-based classification and regression models. However, they are rarely used and considered in numerical analysis due to their lack of strict positive definiteness. In particular they do not enjoy the usual property of unisolvency for arbitrary point sets, which is one of the key properties used to build kernel-based interpolation methods. This work is devoted to establish some initial results for the study of these kernels, and their related interpolation algorithms, in the context of approximation theory. We will first prove necessary and sufficient conditions on point sets which guarantee the existence and uniqueness of an interpolant. We will then study the Reproducing Kernel Hilbert Spaces (or native spaces) of these kernels and their norms, and provide inclusion relations between spaces corresponding to different kernel parameters. With these spaces at hand, it will be further possible to derive generic error estimates which apply to sufficiently smooth functions, thus escaping the native space. Finally, we will show how to employ an efficient stable algorithm to these kernels to obtain accurate interpolants, and we will test them in some numerical experiments.
<br> 
<a href='https://GabrieleSantin.github.io/files/2023_ATMA_poster.html'> <i class='fa fa-file-pdf'></i> Lecture notes</a> 


</td>

</tr>


</table>
</font>
